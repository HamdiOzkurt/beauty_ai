"""
GPU-Accelerated Speech-to-Text Service
Faster-Whisper ile CUDA destekli hÄ±zlÄ± transkripsiyon
"""

# âš ï¸ KRÄ°TÄ°K: CUDA/cuDNN ortamÄ±nÄ± hazÄ±rla - TÃœM import'lardan Ã–NCE!
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# 1) PATH ayarlarÄ±
import cuda_setup

# 2) DLL'leri Ã¶nceden yÃ¼kle
import cudnn_preload

import warnings

# cuDNN / CUDA ile ilgili uyarÄ±larÄ± kÄ±s
warnings.filterwarnings("ignore", category=UserWarning)

# CUDA modÃ¼llerini lazy yÃ¼kle (Whisper/Faster-Whisper Ã¶nerisi)
os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "3")
os.environ.setdefault("CUDA_MODULE_LOADING", "LAZY")


# ==================================================
# KÃœTÃœPHANE Ä°THALATLARI (DLL yollarÄ± ayarlandÄ±ktan sonra)
# ==================================================
import time
import logging
import numpy as np
from faster_whisper import WhisperModel
import io
import wave
import torch

# ==========================================
# GPU HIZ OPTÄ°MÄ°ZASYONU AYARLARI
# ==========================================

# FFmpeg yolu (Windows iÃ§in sabit path)
FFMPEG_PATH = r"C:\Users\hamdi\Downloads\ffmpeg-8.0-full_build\ffmpeg-8.0-full_build\bin\ffmpeg.exe"

# Model Boyutu: 'tiny', 'base', 'small', 'medium', 'large-v2', 'large-v3'
# TÃ¼rkÃ§e iÃ§in en iyi: large-v3 (en aÄŸÄ±r ama maksimum doÄŸruluk)
MODEL_SIZE = "small"

# Hesaplama Tipi: RTX serisi iÃ§in "float16", eski kartlar iÃ§in "int8"
# cuDNN hatasÄ± varsa "int8" kullan (yine de GPU hÄ±zlÄ± Ã§alÄ±ÅŸÄ±r)
COMPUTE_TYPE = "int8"  # Genel amaÃ§lÄ± compute tipi (bilgi amaÃ§lÄ±)

# Daha ince ayar iÃ§in ayrÄ± GPU/CPU compute tipleri
GPU_COMPUTE_TYPE = "float16"  # GPU'da daha doÄŸal ve doÄŸru sonuÃ§lar
CPU_COMPUTE_TYPE = "int8"     # CPU fallback iÃ§in hafif ve hÄ±zlÄ±

# Cihaz: GPU iÃ§in "cuda", CPU iÃ§in "cpu"
DEVICE = "cuda"

class GPUWhisperSTT:
    """GPU hÄ±zlandÄ±rmalÄ± Whisper STT servisi"""
    
    def __init__(self):
        """Faster-Whisper ve VAD modellerini GPU ile yÃ¼kle"""
        self.model = None
        self.vad_model = None
        self.vad_utils = None
        
        self._load_model()
        self._load_vad_model()
    
    def _load_vad_model(self):
        """Silero VAD modelini yÃ¼kle."""
        try:
            logging.info("ğŸ”Š VAD modeli yÃ¼kleniyor (silero-vad)...")
            model, utils = torch.hub.load(
                repo_or_dir='snakers4/silero-vad',
                model='silero_vad',
                force_reload=False,
                onnx=False  # ONNX sÃ¼rÃ¼mÃ¼ CPU'da daha iyi, biz PyTorch istiyoruz
            )
            self.vad_model = model
            self.vad_utils = utils
            logging.info("âœ… VAD modeli baÅŸarÄ±yla yÃ¼klendi.")
        except Exception as e:
            logging.error(f"âŒ VAD modeli yÃ¼klenemedi: {e}")
            # VAD olmadan devam edilebilir ama streaming Ã§alÄ±ÅŸmaz.
            # Åimdilik hata verip durdurmak yerine sadece uyarÄ±yoruz.
            self.vad_model = None
            self.vad_utils = None
            
    def _load_model(self):
        """Modeli yÃ¼kle - Ã¶nce kalite, sonra hÄ±z odaklÄ± fallback ile."""

        # 1) Tercih: GPU + float16 (RTX 4050 iÃ§in ideal)
        try: 
            logging.info(f"ğŸš€ Model yÃ¼kleniyor: CUDA - {GPU_COMPUTE_TYPE} ({MODEL_SIZE})...")
            
            self.model = WhisperModel(
                MODEL_SIZE, 
                device="cuda", 
                compute_type=GPU_COMPUTE_TYPE,
                num_workers=2,
                cpu_threads=4,
                download_root=None
            )
            
            logging.info(f"âœ… Model baÅŸarÄ±yla yÃ¼klendi: CUDA ({MODEL_SIZE}/{GPU_COMPUTE_TYPE})")
            return
            
        except Exception as e:
            logging.warning(f"[WARN] CUDA/{GPU_COMPUTE_TYPE} yuklenemedi: {str(e)[:120]}")
            logging.info("[INFO] CPU moduna geciliyor...")
            
            try:
                self.model = WhisperModel(
                    MODEL_SIZE, 
                    device="cpu", 
                    compute_type=CPU_COMPUTE_TYPE,
                    num_workers=2,
                    download_root=None
                )
                logging.info(f"âœ… Model baÅŸarÄ±yla yÃ¼klendi: CPU ({MODEL_SIZE}/{CPU_COMPUTE_TYPE})")
                return
            except Exception as e2:
                raise RuntimeError(f"âŒ Model yÃ¼klenemedi: {str(e2)}")
    
    def transcribe_audio_bytes(self, audio_bytes: bytes, language: str = "tr") -> tuple:
        """
        Ses verisini metne Ã§evir (GPU hÄ±zlandÄ±rmalÄ±)
        
        Args:
            audio_bytes: WebM/WAV/MP3 formatÄ±nda ses verisi
            language: Dil kodu (varsayÄ±lan: "tr")
            
        Returns:
            tuple: (metin, iÅŸlem sÃ¼resi)
        """
        if not self.model:
            raise RuntimeError("Whisper modeli yÃ¼klenmemiÅŸ!")
        
        start_time = time.time()
        
        try:
            # WebM â†’ WAV dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (tarayÄ±cÄ±dan gelen ses kalitesini iyileÅŸtir)
            import tempfile
            import subprocess
            
            with tempfile.NamedTemporaryFile(suffix=".webm", delete=False) as temp_input:
                temp_input.write(audio_bytes)
                temp_input_path = temp_input.name
            
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_output:
                temp_output_path = temp_output.name
            
            try:
                # FFmpeg ile yÃ¼ksek kaliteli WAV'a Ã§evir
                subprocess.run([
                    FFMPEG_PATH, "-y", "-i", temp_input_path,
                    "-ar", "16000",  # 16kHz sampling rate (Whisper iÃ§in optimal)
                    "-ac", "1",       # Mono
                    "-c:a", "pcm_s16le",  # 16-bit PCM
                    temp_output_path
                ], check=True, capture_output=True)
                
                # DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ WAV'Ä± oku
                with open(temp_output_path, "rb") as f:
                    audio_file = io.BytesIO(f.read())
            finally:
                # GeÃ§ici dosyalarÄ± temizle
                try:
                    os.unlink(temp_input_path)
                    os.unlink(temp_output_path)
                except:
                    pass
            
            # --- MAKSIMUM KALÄ°TE TRANSKRÄ°PSÄ°YON (TÃ¼rkÃ§e optimizasyonu) ---
            segments, info = self.model.transcribe(
                audio_file,
                language=language,      # TÃ¼rkÃ§e sabit
                beam_size=2,            # Beam search: en iyi 5 yolu tara
                best_of=3,              # Her segment iÃ§in 5 deneme, en iyisini seÃ§
                temperature=0.0,        # Deterministik Ã§Ä±ktÄ±
                patience=2.0,           # Daha sabÄ±rlÄ± decode (kalite iÃ§in)
                length_penalty=1.0,     # Uzun cÃ¼mleleri penalize etme
                repetition_penalty=1.1, # Tekrar eden kelimeleri hafifÃ§e engelleyip
                vad_filter=True,        # Sessiz kÄ±sÄ±mlarÄ± atla
                vad_parameters=dict(
                    min_silence_duration_ms=700,  # CÃ¼mle sonunu daha iyi yakala
                    threshold=0.5,
                    min_speech_duration_ms=250
                ),
                condition_on_previous_text=True,  # Ã–nceki baÄŸlamÄ± kullan (cÃ¼mle tutarlÄ±lÄ±ÄŸÄ±)
                initial_prompt="Merhaba, randevu almak istiyorum. YarÄ±n iÃ§in mÃ¼sait misiniz?",  # TÃ¼rkÃ§e kontext
                no_speech_threshold=0.6,
                log_prob_threshold=-1.0,
                compression_ratio_threshold=2.4
            )
            
            # Segmentleri birleÅŸtir
            text = " ".join([segment.text.strip() for segment in segments])
            
            end_time = time.time()
            process_time = end_time - start_time
            
            # DetaylÄ± log
            logging.info(f"ğŸ¤ STT: '{text[:50]}...' ({process_time:.2f}s - {info.language})")
            
            return text.strip(), process_time
            
        except Exception as e:
            logging.error(f"âŒ Transkripsiyon hatasÄ±: {e}")
            raise
    
    def transcribe_audio_file(self, file_path: str, language: str = "tr") -> tuple:
        """
        Dosyadan ses Ã§evir
        
        Args:
            file_path: Ses dosyasÄ± yolu
            language: Dil kodu
            
        Returns:
            tuple: (metin, iÅŸlem sÃ¼resi)
        """
        start_time = time.time()
        
        try:
            segments, info = self.model.transcribe(
                file_path,
                language=language,
                beam_size=1,
                vad_filter=True,
                temperature=0.0,
                condition_on_previous_text=False
            )
            
            text = " ".join([segment.text.strip() for segment in segments])
            process_time = time.time() - start_time
            
            logging.info(f"ğŸ¤ STT (dosya): '{text[:50]}...' ({process_time:.2f}s)")
            
            return text.strip(), process_time
            
        except Exception as e:
            logging.error(f"âŒ Dosya transkripsiyon hatasÄ±: {e}")
            raise

    def transcribe_tensor(self, audio_tensor, language: str = "tr") -> tuple:
        """
        Bir ses tensÃ¶rÃ¼nÃ¼ (veya numpy dizisini) doÄŸrudan transkribe eder.
        Streaming iÃ§in optimize edilmiÅŸtir, FFmpeg dÃ¶nÃ¼ÅŸÃ¼mÃ¼ yapmaz.
        """
        if not self.model:
            raise RuntimeError("Whisper modeli yÃ¼klenmemiÅŸ!")

        start_time = time.time()
        
        try:
            # --- STREAMING Ä°Ã‡Ä°N OPTÄ°MÄ°ZE EDÄ°LMÄ°Å TRANSKRÄ°PSÄ°YON ---
            # VAD filtresi burada harici olarak yapÄ±ldÄ±ÄŸÄ± iÃ§in kapatÄ±labilir.
            # Ancak yine de iÃ§erideki VAD'nin kÃ¼Ã§Ã¼k sessizlikleri temizlemesi faydalÄ± olabilir.
            segments, info = self.model.transcribe(
                audio_tensor,
                language=language,
                beam_size=5,
                temperature=0.0,
                condition_on_previous_text=True,
                initial_prompt="Merhaba, randevu almak istiyorum. YarÄ±n iÃ§in mÃ¼sait misiniz?",
                vad_filter=True,
                vad_parameters=dict(min_silence_duration_ms=100) # Ä°Ã§ VAD iÃ§in daha agresif ayar
            )
            
            text = " ".join([segment.text.strip() for segment in segments])
            process_time = time.time() - start_time
            
            logging.info(f"ğŸ¤ STT (Stream): '{text[:50]}...' ({process_time:.2f}s - {info.language})")
            
            return text.strip(), process_time

        except Exception as e:
            logging.error(f"âŒ Tensor transkripsiyon hatasÄ±: {e}")
            raise

    def create_audio_processor(self, **kwargs):
        """Streaming iÃ§in bir AudioProcessor nesnesi oluÅŸturur."""
        if not self.vad_model:
            raise RuntimeError("VAD modeli yÃ¼klenemediÄŸi iÃ§in streaming processor oluÅŸturulamÄ±yor.")
        return AudioProcessor(stt_service=self, **kwargs)


class AudioProcessor:
    """
    GerÃ§ek zamanlÄ± ses akÄ±ÅŸÄ±nÄ± iÅŸler, VAD kullanarak konuÅŸmayÄ± algÄ±lar,
    biriktirir ve transkripsiyon iÃ§in GPUWhisperSTT'ye gÃ¶nderir.
    """
    def __init__(self, stt_service: GPUWhisperSTT, 
                 vad_threshold: float = 0.5, 
                 min_silence_duration_ms: int = 300, # Daha hassas ayar
                 min_speech_duration_ms: int = 100, # KÄ±sa sesleri de yakala
                 sampling_rate: int = 16000):
        
        self.stt_service = stt_service
        self.vad_threshold = vad_threshold
        self.min_silence_duration_ms = min_silence_duration_ms
        self.min_speech_duration_ms = min_speech_duration_ms
        self.sampling_rate = sampling_rate

        self._reset_stream()

    def _reset_stream(self):
        """AkÄ±ÅŸ durumunu ve buffer'Ä± sÄ±fÄ±rla."""
        logging.debug("[RESET] Akis sifirlaniyor...")
        self.audio_buffer = []
        self.speaking = False
        self.silence_frames = 0
        self.speech_frames = 0

    def process_chunk(self, chunk: bytes):
        """
        Gelen ses parÃ§asÄ±nÄ± (chunk) iÅŸle.
        KonuÅŸma algÄ±larsa buffer'a ekler.
        Sessizlik algÄ±larsa ve buffer doluysa transkripsiyonu tetikler.
        """
        # Gelen chunk'Ä± PyTorch tensor'a Ã§evir
        # Silero VAD 1D tensor bekler
        audio_float32 = np.frombuffer(chunk, dtype=np.int16).astype(np.float32) / 32768.0
        audio_tensor = torch.from_numpy(audio_float32)

        if audio_tensor.numel() == 0:
            return None # BoÅŸ chunk'Ä± atla

        # VAD ile konuÅŸma olasÄ±lÄ±ÄŸÄ±nÄ± hesapla
        speech_prob = self.stt_service.vad_model(audio_tensor, self.sampling_rate).item()

        chunk_duration_ms = (len(chunk) / 2) / self.sampling_rate * 1000

        if speech_prob > self.vad_threshold:
            # KonuÅŸma algÄ±landÄ±
            self.silence_frames = 0
            if not self.speaking:
                logging.info("â–¶ï¸ KonuÅŸma baÅŸladÄ±.")
                self.speaking = True
            
            self.speech_frames += 1
            self.audio_buffer.append(audio_tensor)
            return None # HenÃ¼z transkript yok
        else:
            # Sessizlik algÄ±landÄ±
            if self.speaking:
                self.silence_frames += 1
                total_silence_ms = self.silence_frames * chunk_duration_ms

                if total_silence_ms >= self.min_silence_duration_ms:
                    logging.info(f"â¹ï¸ KonuÅŸma bitti ({total_silence_ms:.0f}ms sessizlik). Transkripsiyon tetikleniyor.")
                    
                    full_audio = torch.cat(self.audio_buffer)
                    
                    # KonuÅŸma Ã§ok kÄ±saysa (gÃ¼rÃ¼ltÃ¼ olabilir), iÅŸlemi atla
                    total_speech_ms = self.speech_frames * chunk_duration_ms
                    if total_speech_ms < self.min_speech_duration_ms:
                        logging.info(f"â­ï¸  KonuÅŸma Ã§ok kÄ±sa ({total_speech_ms:.0f}ms), gÃ¼rÃ¼ltÃ¼ olarak kabul edildi ve atlandÄ±.")
                        self._reset_stream()
                        return None

                    # Buffer'daki sesi birleÅŸtir ve transkribe et
                    try:
                        transcript, _ = self.stt_service.transcribe_tensor(full_audio.numpy())
                        self._reset_stream()
                        return transcript
                    except Exception as e:
                        logging.error(f"STREAMING TRANSCRIBE ERROR: {e}")
                        self._reset_stream()
                        return None
            
            return None # Sessizlik devam ediyor veya konuÅŸma hiÃ§ baÅŸlamadÄ±


# Global STT instance (singleton pattern - lazy load)
_stt_service = None

def get_stt_service() -> GPUWhisperSTT:
    """STT service'i al (singleton)"""
    global _stt_service
    if _stt_service is None:
        _stt_service = GPUWhisperSTT()
    return _stt_service


# Test fonksiyonu
if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    print("=" * 60)
    print("ğŸ¤ GPU Whisper STT Servisi Test")
    print("=" * 60)
    
    stt = get_stt_service()
    
    print("\nâœ… STT servisi hazÄ±r ve kullanÄ±ma aÃ§Ä±k!")
    print(f"   ğŸ“Š Model Bilgileri:")
    print(f"   - Model: {MODEL_SIZE}")
    print(f"   - Cihaz: {DEVICE}")
    print(f"   - Compute: {COMPUTE_TYPE}")
    print("\nğŸ’¡ KullanÄ±m:")
    print("   from stt_service_gpu import get_stt_service")
    print("   stt = get_stt_service()")
    print("   text, duration = stt.transcribe_audio_bytes(audio_bytes)")
    print("\nğŸ™ï¸  Mikrofon testi iÃ§in: python test_gpu_stt.py")
    print("=" * 60)
